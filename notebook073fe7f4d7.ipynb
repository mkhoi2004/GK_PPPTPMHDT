{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process raw data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python -m venv myenv\n",
    "\n",
    "1️⃣ Kích hoạt venv trong Terminal\n",
    "myenv\\Scripts\\activate\n",
    "\n",
    "2️⃣ Cài thư viện bằng pip trong Terminal\n",
    "\n",
    "pip install numpy opencv-python tensorflow scikit-learn notebook ipykernel\n",
    "\n",
    "3️⃣ Đăng ký kernel venv với Jupyter\n",
    "\n",
    "python -m ipykernel install --user --name=myenv --display-name \"Python (myenv)\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T12:14:34.377049Z",
     "iopub.status.busy": "2025-07-14T12:14:34.376324Z",
     "iopub.status.idle": "2025-07-14T12:14:34.381358Z",
     "shell.execute_reply": "2025-07-14T12:14:34.380357Z",
     "shell.execute_reply.started": "2025-07-14T12:14:34.37701Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-14T12:14:41.041917Z",
     "iopub.status.busy": "2025-07-14T12:14:41.041585Z",
     "iopub.status.idle": "2025-07-14T12:14:41.04638Z",
     "shell.execute_reply": "2025-07-14T12:14:41.045556Z",
     "shell.execute_reply.started": "2025-07-14T12:14:41.041891Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Path of the directory with raw images\n",
    "raw_dir = 'archive/train'\n",
    "folders = ['Open_Eyes', 'Closed_Eyes']\n",
    "\n",
    "# Path of the directory where processed images will be stored\n",
    "processed_dir = 'working'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T12:14:42.68539Z",
     "iopub.status.busy": "2025-07-14T12:14:42.685098Z",
     "iopub.status.idle": "2025-07-14T12:15:23.329689Z",
     "shell.execute_reply": "2025-07-14T12:15:23.32891Z",
     "shell.execute_reply.started": "2025-07-14T12:14:42.685369Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Mean and std of ImageNet will be used to normalize the images\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std  = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "def process_image(img_path):\n",
    "    img = cv.imread(img_path)\n",
    "    if img is None:\n",
    "        print(f\"Error loading {img_path}.\")\n",
    "        return None\n",
    "    # Resize image to 224 x 224 as this image size is expected by ResNet or MobileNet which will be used for image classification\n",
    "    img = cv.resize(img, (224, 224))\n",
    "    # cv2 uses the BGR color format, so we convert it to the RGB format\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "    # Normalize the image values to range [0, 1]\n",
    "    img = img / 255.0\n",
    "    img = (img - mean) / std\n",
    "    return img\n",
    "\n",
    "\n",
    "for label, folder in enumerate(folders):\n",
    "    folder_path = os.path.join(raw_dir, folder)\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        img = process_image(file_path)\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "            labels.append(label)\n",
    "\n",
    "# Convert images and labels to NumPy arrays\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Save processed images and labels\n",
    "np.save(os.path.join(processed_dir, \"images.npy\"), images)\n",
    "np.save(os.path.join(processed_dir, \"labels.npy\"), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T12:16:03.851422Z",
     "iopub.status.busy": "2025-07-14T12:16:03.851129Z",
     "iopub.status.idle": "2025-07-14T12:16:14.074932Z",
     "shell.execute_reply": "2025-07-14T12:16:14.072433Z",
     "shell.execute_reply.started": "2025-07-14T12:16:03.8514Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 2400\n",
      "Validation set size: 800\n",
      "Test set size: 800\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the processed dataset\n",
    "images = np.load(os.path.join(processed_dir, \"images.npy\"))\n",
    "labels = np.load(os.path.join(processed_dir, \"labels.npy\"))\n",
    "\n",
    "'''Split the dataset into train+val (80%) and test datasets (20%). The data is automatically shuffled. \n",
    "stratify=labels: the generated splits gave the same proprotion of labels as given by parameter \"labels\".'''\n",
    "images_train_val, images_test, labels_train_val, labels_test = train_test_split(images, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "# Split the train_val dataset into train (75%) and val datasets (25%)\n",
    "images_train, images_val, labels_train, labels_val = train_test_split(images_train_val, labels_train_val, test_size=0.25, random_state=42, stratify=labels_train_val)\n",
    "\n",
    "print(f\"Train set size: {images_train.shape[0]}\")\n",
    "print(f\"Validation set size: {images_val.shape[0]}\")\n",
    "print(f\"Test set size: {images_test.shape[0]}\")\n",
    "\n",
    "# Save split data and labels\n",
    "np.save(os.path.join(processed_dir, \"train_images.npy\"), images_train)\n",
    "np.save(os.path.join(processed_dir, \"train_labels.npy\"), labels_train)\n",
    "np.save(os.path.join(processed_dir, \"val_images.npy\"), images_val)\n",
    "np.save(os.path.join(processed_dir, \"val_labels.npy\"), labels_val)\n",
    "np.save(os.path.join(processed_dir, \"test_images.npy\"), images_test)\n",
    "np.save(os.path.join(processed_dir, \"test_labels.npy\"), labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T12:19:14.030067Z",
     "iopub.status.busy": "2025-07-14T12:19:14.029419Z",
     "iopub.status.idle": "2025-07-14T12:19:14.035827Z",
     "shell.execute_reply": "2025-07-14T12:19:14.035127Z",
     "shell.execute_reply.started": "2025-07-14T12:19:14.03004Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras import models, layers, Sequential\n",
    "\n",
    "def get_resnet50_model(variant='base'):\n",
    "    # Load the ResNet50 model pretrained on ImageNet\n",
    "    base_model = ResNet50(include_top=False, input_shape=(224, 224, 3))\n",
    "    train_from = 150\n",
    "\n",
    "    if variant == 'finetune':\n",
    "        base_model.trainable = True\n",
    "        for layer in base_model.layers[:train_from]:\n",
    "            layer.trainable = False\n",
    "    else:\n",
    "        # Freeze all layers\n",
    "        base_model.trainable = False\n",
    "    \n",
    "    model_append = [layers.GlobalAveragePooling2D()]\n",
    "    model_append.append(layers.Dense(1, activation='sigmoid'))\n",
    "    model = Sequential([base_model] + model_append)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T12:19:47.243032Z",
     "iopub.status.busy": "2025-07-14T12:19:47.242398Z",
     "iopub.status.idle": "2025-07-14T12:23:00.795112Z",
     "shell.execute_reply": "2025-07-14T12:23:00.794401Z",
     "shell.execute_reply.started": "2025-07-14T12:19:47.24301Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8s/step - auc: 0.9601 - binary_accuracy: 0.8942 - f1_score: 0.8988 - loss: 0.2075 - precision: 0.8772 - recall: 0.9261"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m857s\u001b[0m 11s/step - auc: 0.9944 - binary_accuracy: 0.9579 - f1_score: 0.9581 - loss: 0.0963 - precision: 0.9530 - recall: 0.9633 - val_auc: 0.9976 - val_binary_accuracy: 0.5213 - val_f1_score: 0.0815 - val_loss: 0.5608 - val_precision: 1.0000 - val_recall: 0.0425\n",
      "Epoch 2/15\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8s/step - auc: 0.9995 - binary_accuracy: 0.9861 - f1_score: 0.9852 - loss: 0.0306 - precision: 0.9810 - recall: 0.9900"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m820s\u001b[0m 11s/step - auc: 0.9996 - binary_accuracy: 0.9875 - f1_score: 0.9875 - loss: 0.0285 - precision: 0.9875 - recall: 0.9875 - val_auc: 0.9992 - val_binary_accuracy: 0.8400 - val_f1_score: 0.8095 - val_loss: 0.3200 - val_precision: 1.0000 - val_recall: 0.6800\n",
      "Epoch 3/15\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m663s\u001b[0m 9s/step - auc: 0.9995 - binary_accuracy: 0.9975 - f1_score: 0.9975 - loss: 0.0112 - precision: 0.9967 - recall: 0.9983 - val_auc: 0.9875 - val_binary_accuracy: 0.7075 - val_f1_score: 0.5866 - val_loss: 0.6838 - val_precision: 1.0000 - val_recall: 0.4150\n",
      "Epoch 4/15\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7s/step - auc: 1.0000 - binary_accuracy: 0.9977 - f1_score: 0.9977 - loss: 0.0070 - precision: 0.9973 - recall: 0.9981"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m615s\u001b[0m 8s/step - auc: 0.9999 - binary_accuracy: 0.9967 - f1_score: 0.9967 - loss: 0.0111 - precision: 0.9958 - recall: 0.9975 - val_auc: 0.9975 - val_binary_accuracy: 0.9312 - val_f1_score: 0.9357 - val_loss: 0.1612 - val_precision: 0.8791 - val_recall: 1.0000\n",
      "Epoch 5/15\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m789s\u001b[0m 11s/step - auc: 1.0000 - binary_accuracy: 0.9996 - f1_score: 0.9996 - loss: 0.0040 - precision: 0.9992 - recall: 1.0000 - val_auc: 0.9187 - val_binary_accuracy: 0.6562 - val_f1_score: 0.4762 - val_loss: 1.3627 - val_precision: 1.0000 - val_recall: 0.3125\n",
      "Epoch 6/15\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9s/step - auc: 1.0000 - binary_accuracy: 1.0000 - f1_score: 1.0000 - loss: 6.9346e-04 - precision: 1.0000 - recall: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m902s\u001b[0m 12s/step - auc: 1.0000 - binary_accuracy: 1.0000 - f1_score: 1.0000 - loss: 5.8723e-04 - precision: 1.0000 - recall: 1.0000 - val_auc: 1.0000 - val_binary_accuracy: 0.9950 - val_f1_score: 0.9950 - val_loss: 0.0189 - val_precision: 1.0000 - val_recall: 0.9900\n",
      "Epoch 7/15\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8s/step - auc: 1.0000 - binary_accuracy: 1.0000 - f1_score: 1.0000 - loss: 3.6862e-04 - precision: 1.0000 - recall: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m837s\u001b[0m 11s/step - auc: 1.0000 - binary_accuracy: 1.0000 - f1_score: 1.0000 - loss: 4.0995e-04 - precision: 1.0000 - recall: 1.0000 - val_auc: 1.0000 - val_binary_accuracy: 0.9975 - val_f1_score: 0.9975 - val_loss: 0.0088 - val_precision: 0.9950 - val_recall: 1.0000\n",
      "Epoch 8/15\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m739s\u001b[0m 10s/step - auc: 0.9997 - binary_accuracy: 0.9942 - f1_score: 0.9942 - loss: 0.0200 - precision: 0.9933 - recall: 0.9950 - val_auc: 0.9987 - val_binary_accuracy: 0.9937 - val_f1_score: 0.9937 - val_loss: 0.0220 - val_precision: 0.9950 - val_recall: 0.9925\n",
      "Epoch 9/15\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7s/step - auc: 1.0000 - binary_accuracy: 0.9990 - f1_score: 0.9990 - loss: 0.0044 - precision: 0.9981 - recall: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m695s\u001b[0m 9s/step - auc: 1.0000 - binary_accuracy: 0.9996 - f1_score: 0.9996 - loss: 0.0031 - precision: 0.9992 - recall: 1.0000 - val_auc: 1.0000 - val_binary_accuracy: 0.9975 - val_f1_score: 0.9975 - val_loss: 0.0051 - val_precision: 1.0000 - val_recall: 0.9950\n",
      "Epoch 10/15\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m757s\u001b[0m 10s/step - auc: 1.0000 - binary_accuracy: 1.0000 - f1_score: 1.0000 - loss: 0.0015 - precision: 1.0000 - recall: 1.0000 - val_auc: 0.9975 - val_binary_accuracy: 0.9550 - val_f1_score: 0.9569 - val_loss: 0.1155 - val_precision: 0.9174 - val_recall: 1.0000\n",
      "Epoch 11/15\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m756s\u001b[0m 10s/step - auc: 1.0000 - binary_accuracy: 1.0000 - f1_score: 1.0000 - loss: 3.0787e-04 - precision: 1.0000 - recall: 1.0000 - val_auc: 1.0000 - val_binary_accuracy: 0.9975 - val_f1_score: 0.9975 - val_loss: 0.0117 - val_precision: 0.9950 - val_recall: 1.0000\n",
      "Epoch 12/15\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m718s\u001b[0m 10s/step - auc: 1.0000 - binary_accuracy: 1.0000 - f1_score: 1.0000 - loss: 1.9213e-04 - precision: 1.0000 - recall: 1.0000 - val_auc: 1.0000 - val_binary_accuracy: 0.9975 - val_f1_score: 0.9975 - val_loss: 0.0060 - val_precision: 0.9950 - val_recall: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import BinaryAccuracy, Precision, Recall, AUC, F1Score\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "\n",
    "# Load train dataset\n",
    "X_train = np.load(os.path.join(processed_dir, \"train_images.npy\"))\n",
    "y_train = np.load(os.path.join(processed_dir, \"train_labels.npy\"))\n",
    "# Reshape y_train (num_train_examples,) to have the same shape as model predictions in tf (num_train_examples, 1)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "\n",
    "# Load validation dataset\n",
    "X_val = np.load(os.path.join(processed_dir, \"val_images.npy\"))\n",
    "y_val = np.load(os.path.join(processed_dir, \"val_labels.npy\"))\n",
    "# Reshape y_val (num_val_examples,) to have the same shape as model predictions in tf (num_val_examples, 1)\n",
    "y_val = y_val.reshape(-1, 1)\n",
    "\n",
    "# Choose model variant: 'base', 'finetune'\n",
    "model_variant = 'finetune'\n",
    "# Load ResNet50 model\n",
    "model = get_resnet50_model(variant=model_variant)\n",
    "\n",
    "# Configure model settings for training\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4), loss=BinaryCrossentropy(), metrics=[BinaryAccuracy(), Precision(), Recall(), AUC(), F1Score(threshold=0.5)])\n",
    "\n",
    "# Callbacks\n",
    "# Create the EarlyStopping callback\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint(f\"working/{model_variant}.h5\", save_best_only=True)\n",
    "\n",
    "# Train modified ResNet50\n",
    "history = model.fit(X_train, y_train, validation_data=[X_val, y_val], batch_size=32, epochs=15, callbacks=[early_stop, checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T12:24:51.755301Z",
     "iopub.status.busy": "2025-07-14T12:24:51.754989Z",
     "iopub.status.idle": "2025-07-14T12:25:14.014057Z",
     "shell.execute_reply": "2025-07-14T12:25:14.01326Z",
     "shell.execute_reply.started": "2025-07-14T12:24:51.755284Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 7s/step - auc: 1.0000 - binary_accuracy: 0.9987 - f1_score: 0.9987 - loss: 0.0054 - precision: 1.0000 - recall: 0.9975\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 7s/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Open       1.00      1.00      1.00       400\n",
      "      Closed       1.00      1.00      1.00       400\n",
      "\n",
      "    accuracy                           1.00       800\n",
      "   macro avg       1.00      1.00      1.00       800\n",
      "weighted avg       1.00      1.00      1.00       800\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[400   0]\n",
      " [  1 399]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Load test dataset\n",
    "X_test = np.load(\"working/test_images.npy\")\n",
    "y_test = np.load(\"working/test_labels.npy\")\n",
    "# Reshape y_test (num_test_examples,) to have the same shape as model predictions in tf (num_test_examples, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "# Load the updated resnet model\n",
    "model = load_model(\"working/finetune.h5\")\n",
    "\n",
    "# Evaluate on the test dataset\n",
    "results = model.evaluate(X_test, y_test, batch_size=32, verbose=1)\n",
    "\n",
    "# Predict labels\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Open\", \"Closed\"]))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1048759,
     "sourceId": 1770390,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "myenv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
